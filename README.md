# ğŸ” Test Pattern Detection Tool

**Professional automation tool for detecting flaky and consistently failing tests**

[![Version](https://img.shields.io/badge/version-1.0.0-blue.svg)](https://github.com/optisol)
[![Python](https://img.shields.io/badge/python-3.7+-green.svg)](https://python.org)
[![License](https://img.shields.io/badge/license-MIT-orange.svg)](LICENSE)

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Usage](#usage)
- [Output Reports](#output-reports)
- [CI/CD Integration](#cicd-integration)
- [Methodology](#methodology)
- [Configuration](#configuration)
- [Troubleshooting](#troubleshooting)

---

## ğŸ¯ Overview

The Test Pattern Detection Tool is an enterprise-grade solution for identifying problematic tests in your test suites. It automatically analyzes test execution history to classify tests as:

- âœ… **Stable** - Consistently passing tests
- âš ï¸ **Flaky** - Tests that fail intermittently
- âŒ **Consistently Failing** - Tests that always fail
- ğŸ“Š **Mostly Stable/Failing** - Tests with trending patterns

### Why This Matters

- **Save Time**: Stop investigating false test failures
- **Improve Reliability**: Identify and fix flaky tests
- **Prioritize Work**: Focus on real bugs vs. test issues
- **Increase Confidence**: Build trust in your CI/CD pipeline

---

## âœ¨ Features

### Core Capabilities

- ğŸ¤– **Automated Pattern Detection**: No manual analysis needed
- ğŸ“Š **Smart Classification**: Uses statistical algorithms with confidence scoring
- ğŸ“ˆ **Interactive Dashboard**: Beautiful HTML visualizations
- ğŸ“ **Multiple Output Formats**: CSV, JSON, HTML
- ğŸ”§ **Configurable Thresholds**: Customize for your needs
- ğŸš€ **CI/CD Ready**: Integrate with Jenkins, GitLab, GitHub Actions
- ğŸ“ **Detailed Logging**: Full audit trail of analysis

### Professional Features

- Error pattern clustering
- Execution duration analysis
- Confidence scoring system
- Health score calculation
- Trend visualization
- Priority-based reporting

---

## ğŸš€ Installation

### Prerequisites

- Python 3.7 or higher
- No external dependencies (uses standard library only!)

### Setup

```bash
# Clone or download the tool
git clone <repository-url>
cd TestPatternDetector

# Make the script executable (Unix/Linux/Mac)
chmod +x detector.py generate_sample_data.py

# No pip install needed! Uses Python standard library only.
```

---

## âš¡ Quick Start

### Step 1: Generate Sample Data (Optional)

```bash
python generate_sample_data.py
```

This creates 15 sample test runs in `sample_data/` directory.

### Step 2: Run Analysis

```bash
python detector.py --input sample_data/*.xml
```

### Step 3: View Results

Open `output/dashboard.html` in your browser to see the interactive dashboard!

---

## ğŸ“– Usage

### Basic Usage

```bash
python detector.py --input <test_results_files>
```

### Advanced Usage

```bash
python detector.py \
  --input test_run_1.xml test_run_2.xml test_run_3.xml \
  --min-runs 10 \
  --output-dir my_reports
```

### Command Line Options

| Option | Description | Default |
|--------|-------------|---------|
| `--input` | Test result files (JUnit XML) | Required |
| `--min-runs` | Minimum runs for classification | 5 |
| `--output-dir` | Output directory for reports | output |

### Supported Input Formats

The tool supports **JUnit XML** format, which is generated by most testing frameworks:

- **Java**: JUnit, TestNG
- **Python**: pytest, unittest
- **JavaScript**: Jest, Mocha
- **C#**: NUnit, xUnit
- **Ruby**: RSpec
- **Go**: go test

---

## ğŸ“Š Output Reports

### 1. Interactive HTML Dashboard (`dashboard.html`)

Professional, responsive dashboard with:
- Summary statistics cards
- Interactive charts (Chart.js)
- Detailed test tables
- Health score visualization

**Features:**
- ğŸ“± Mobile responsive
- ğŸ¨ Beautiful gradient design
- ğŸ“ˆ Real-time chart interactions
- ğŸ” Sortable tables

### 2. CSV Report (`pattern_report.csv`)

Structured data for spreadsheet analysis:

```csv
Test Name,Total Runs,Pass Count,Fail Count,Failure Rate (%),Classification,Confidence Score (%)
test_login,15,15,0,0.0,Stable,100.0
test_api_integration,15,9,6,40.0,Flaky,70.0
test_broken_feature,15,0,15,100.0,Consistently Failing,100.0
```

### 3. JSON Insights (`insights.json`)

Machine-readable format for automation:

```json
{
  "generated_at": "2024-11-18T10:30:00",
  "statistics": {
    "total_tests": 20,
    "stable_tests": 10,
    "flaky_tests": 5,
    "failing_tests": 3
  },
  "test_patterns": [...]
}
```

---

## ğŸ”„ CI/CD Integration

### Jenkins Pipeline

```groovy
stage('Test Pattern Analysis') {
    steps {
        sh '''
            python detector.py \
              --input test-results/*.xml \
              --output-dir reports/patterns
        '''
        
        publishHTML([
            reportName: 'Test Pattern Analysis',
            reportDir: 'reports/patterns',
            reportFiles: 'dashboard.html'
        ])
    }
}
```

### GitHub Actions

```yaml
- name: Analyze Test Patterns
  run: |
    python detector.py --input test-results/*.xml
    
- name: Upload Dashboard
  uses: actions/upload-artifact@v3
  with:
    name: test-pattern-dashboard
    path: output/dashboard.html
```

### GitLab CI

```yaml
test_pattern_analysis:
  script:
    - python detector.py --input test-results/*.xml
  artifacts:
    paths:
      - output/
    reports:
      junit: test-results/*.xml
```

---

## ğŸ§® Methodology

### Classification Algorithm

The tool uses a multi-factor analysis:

1. **Failure Rate Calculation**
   ```
   Failure Rate = (Failed Runs / Total Runs) Ã— 100
   ```

2. **Classification Rules**
   - **Stable**: 0% failure rate
   - **Mostly Stable**: < 5% failure rate
   - **Flaky**: 5-95% failure rate (intermittent)
   - **Mostly Failing**: > 95% failure rate
   - **Consistently Failing**: 100% failure rate

3. **Confidence Scoring**
   - Based on sample size (more runs = higher confidence)
   - Adjusted for failure rate distribution
   - Minimum 5 runs required for reliable classification

### Why 5 Runs Minimum?

With fewer than 5 runs, statistical confidence is too low:
- 3 runs: 33% resolution
- 5 runs: 20% resolution
- 10 runs: 10% resolution âœ… Recommended
- 20+ runs: <5% resolution âœ… High confidence

---

## âš™ï¸ Configuration

### config.yaml

Customize thresholds and behavior:

```yaml
min_runs: 5                    # Minimum runs for classification
flaky_threshold_min: 5         # Minimum % for flaky classification
flaky_threshold_max: 95        # Maximum % for flaky classification
output_directory: "output"     # Output directory
generate_html: true            # Generate HTML dashboard
generate_csv: true             # Generate CSV report
generate_json: true            # Generate JSON insights
```

### Environment-Specific Configurations

**Development Environment**: More tolerant thresholds
```yaml
min_runs: 3
flaky_threshold_min: 10
flaky_threshold_max: 90
```

**Production Environment**: Strict thresholds
```yaml
min_runs: 10
flaky_threshold_min: 2
flaky_threshold_max: 98
```

---

## ğŸ¯ Best Practices

### 1. Data Collection

âœ… **DO:**
- Collect at least 10-20 test runs
- Run tests in consistent environments
- Preserve test history across branches

âŒ **DON'T:**
- Mix test results from different branches
- Analyze less than 5 runs
- Include manual test reruns

### 2. Analysis Frequency

- **Daily**: For active development
- **Weekly**: For stable projects
- **Per-release**: For release validation

### 3. Action Items

**For Flaky Tests:**
1. Review test implementation
2. Check for timing issues
3. Investigate external dependencies
4. Consider test isolation

**For Failing Tests:**
1. Verify recent code changes
2. Check environment configuration
3. Review test data
4. Investigate infrastructure issues

---

## ğŸ”§ Troubleshooting

### Common Issues

#### Issue: "No tests found"

**Solution:**
- Verify XML files are in JUnit format
- Check file paths are correct
- Ensure files contain `<testsuite>` or `<testsuites>` elements

#### Issue: "All tests show 'Insufficient Data'"

**Solution:**
- Provide more test run files (minimum 5 runs)
- Lower `--min-runs` threshold (not recommended for production)

#### Issue: "Charts not displaying in dashboard"

**Solution:**
- Ensure internet connection (Chart.js loads from CDN)
- Check browser console for errors
- Try opening in different browser

---

## ğŸ“ Support

### Contact

- **Developer**: Muji
- **Team**: DevSecOps - Optisol Business Solutions
- **Email**: [muji.shahul@optisol.com]

### Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ™ Acknowledgments

- Optisol Business Solutions - DevSecOps Team
- Mujbur Rahman Sahul Hameed - Program Lead
- Learning and Training Team

---

## ğŸ“Š Sample Output

```
======================================================================
  TEST PATTERN DETECTION TOOL
  Automated Flaky & Failed Test Analysis
  Version 1.0.0 | Optisol Business Solutions
======================================================================

2024-11-18 10:30:00 - INFO - Test Pattern Detector initialized
2024-11-18 10:30:01 - INFO - Parsed 20 test results from sample_data/test_results_run_01.xml
...
2024-11-18 10:30:05 - INFO - Processed 20 unique tests
2024-11-18 10:30:05 - INFO - Pattern analysis complete

======================================================================
  ANALYSIS SUMMARY
======================================================================
  Total Tests Analyzed:      20
  âœ… Stable Tests:           6
  âš ï¸  Flaky Tests:            8
  âŒ Consistently Failing:   2
  ğŸ“Š Mostly Stable:          2
  ğŸ“Š Mostly Failing:         2
  â„¹ï¸  Insufficient Data:     0
======================================================================

======================================================================
  GENERATED REPORTS
======================================================================
  CSV: output/pattern_report.csv
  JSON: output/insights.json
  HTML: output/dashboard.html
======================================================================

  ğŸ¥ TEST SUITE HEALTH SCORE: 65.0%
  âš ï¸  Good, but some tests need attention.

  âœ¨ Analysis complete! Open dashboard.html to view detailed results.
```

---

**Made with â¤ï¸ by Optisol DevSecOps Team**